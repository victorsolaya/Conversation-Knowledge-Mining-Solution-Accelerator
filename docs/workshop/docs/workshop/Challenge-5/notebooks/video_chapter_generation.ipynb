{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11049ef0",
   "metadata": {},
   "source": [
    "# Video Chapters Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccbe11",
   "metadata": {},
   "source": [
    "Generate video chapters based on Azure Content Understanding and Azure OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44bdf4",
   "metadata": {},
   "source": [
    "\n",
    "## Pre-requisites\n",
    "1. Follow [README](../docs/create_azure_ai_service.md) to create essential resource that will be used in this sample.\n",
    "1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefeaab",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "AUTHENTICATION_URL = os.getenv(\"AUTHENTICATION_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7d414",
   "metadata": {},
   "source": [
    "## File to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "VIDEO_LOCATION = Path(\"../data/FlightSimulator.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9abf6",
   "metadata": {},
   "source": [
    "## Create a custom analyzer and submit the video to generate the description\n",
    "The custom analyzer schema is defined in **../analyzer_templates/video_content_understanding.json**. The main custom field is `segmentDescription` as we need to get the descriptions of video segments and feed them into chatGPT to generate the scenes and chapters. Adding transcripts will help to increase the accuracy of scenes/chapters segmentation results. To get transcripts, we will need to set the`returnDetails` parameter in the `config` field to `True`.\n",
    "\n",
    "In this example, we will use the utility class `AzureContentUnderstandingClient` to load the analyzer schema from the template file and submit it to Azure Content Understanding service. Then, we will analyze the video to get the segment descriptions and transcripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e52230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "\n",
    "# add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(\n",
    "    str(parent_dir)\n",
    ")\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "from azure.identity import AzureCliCredential, get_bearer_token_provider\n",
    "credential = AzureCliCredential()\n",
    "token_provider = get_bearer_token_provider(credential, AUTHENTICATION_URL)\n",
    "\n",
    "# The analyzer template is used to define the schema of the output\n",
    "ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_content_understanding.json\"\n",
    "ANALYZER_ID = \"video_scene_chapter\" + \"_\" + str(uuid.uuid4())  # Unique identifier for the analyzer\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/video_chapter_generation\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(\n",
    "    ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85cf38",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video content\n",
    "It might take some time depending on the video length. Try with short videos to get results faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)\n",
    "\n",
    "# Wait for the analysis to complete and get the content analysis result\n",
    "video_cu_result = cu_client.poll_result(\n",
    "    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result)\n",
    "\n",
    "# Optional - Delete the analyzer if it is no longer needed\n",
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95188194",
   "metadata": {},
   "source": [
    "## Aggregate video segments to generate video scenes\n",
    "\n",
    "ChatGPT will be used to combine segment descriptions and transcripts into scenes and provide concise descriptions for each scene.\n",
    "\n",
    "After running this step, you will have a metadata json file of video scenes that can be used to generate video chapters. Each scene has start and end timestamps, short description and corresponding transcripts if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.utility import OpenAIAssistant, generate_scenes\n",
    "\n",
    "# Create an OpenAI Assistant to interact with Azure OpenAI\n",
    "openai_assistant = OpenAIAssistant(\n",
    "    aoai_end_point=AZURE_OPENAI_ENDPOINT,\n",
    "    aoai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    aoai_api_key=None,\n",
    ")\n",
    "\n",
    "# Generate the scenes using the video segment result from Azure Content Understanding\n",
    "scene_result = generate_scenes(video_cu_result, openai_assistant)\n",
    "\n",
    "# Write the scene result to a json file\n",
    "scene_output_json_file = \"./scene_results.json\"\n",
    "with open(scene_output_json_file, \"w\") as f:\n",
    "    f.write(scene_result.model_dump_json(indent=2))\n",
    "    print(f\"Scene result is saved to {scene_output_json_file}\")\n",
    "\n",
    "# Print the scene result for the debugging purpose\n",
    "print(scene_result.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15732fe2",
   "metadata": {},
   "source": [
    "## Create video chapters\n",
    "\n",
    "Create video chapters by combining the video scenes with chatGPT. After running this step, you will have a video chapters json file. Each chapter has start and end timestamps, a title and list of scenes that belong to the chapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f5edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.utility import generate_chapters\n",
    "\n",
    "\n",
    "# Generate the chapters using the scenes result\n",
    "chapter_result = generate_chapters(scene_result, openai_assistant)\n",
    "\n",
    "# Write the chapter result to a json file\n",
    "chapter_output_json_file = \"./chapter_results.json\"\n",
    "with open(chapter_output_json_file, \"w\") as f:\n",
    "    f.write(chapter_result.model_dump_json(indent=2))\n",
    "    print(f\"Chapter result is saved to {chapter_output_json_file}\")\n",
    "\n",
    "# Print out the chapter result for the debugging purpose\n",
    "print(chapter_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
