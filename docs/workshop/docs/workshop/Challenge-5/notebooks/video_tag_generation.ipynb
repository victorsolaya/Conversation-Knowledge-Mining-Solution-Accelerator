{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11049ef0",
   "metadata": {},
   "source": [
    "# Video Tag Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccbe11",
   "metadata": {},
   "source": [
    "Generate video tags based on Azure Content Understanding and Azure OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44bdf4",
   "metadata": {},
   "source": [
    "\n",
    "## Pre-requisites\n",
    "1. Follow [README](../docs/create_azure_ai_service.md) to create essential resource that will be used in this sample.\n",
    "1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefeaab",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2024-12-01-preview\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "AUTHENTICATION_URL = os.getenv(\"AUTHENTICATION_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7d414",
   "metadata": {},
   "source": [
    "## File to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "VIDEO_LOCATION = Path(\"../data/FlightSimulator.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9abf6",
   "metadata": {},
   "source": [
    "## Create a custom analyzer and submit the video to generate tags\n",
    "The custom analyzer schema is defined in **../analyzer_templates/video_tag.json**. The custom fields are `segmentDescription`, `transcript` and `tags`. Adding description and transcripts helps to increase the accuracy of tag generation results. To get transcripts, we will need to set the`returnDetails` parameter in the `config` field to `True`.\n",
    "\n",
    "In this example, we will use the utility class `AzureContentUnderstandingClient` to load the analyzer schema from the template file and submit it to Azure Content Understanding service. Then, we will analyze the video to get the segment tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e52230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "\n",
    "# add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(\n",
    "    str(parent_dir)\n",
    ")\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "from azure.identity import AzureCliCredential, get_bearer_token_provider\n",
    "credential = AzureCliCredential()\n",
    "token_provider = get_bearer_token_provider(credential, AUTHENTICATION_URL)\n",
    "\n",
    "# The analyzer template is used to define the schema of the output\n",
    "ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_tag.json\"\n",
    "ANALYZER_ID = \"video_tag\" + \"_\" + str(uuid.uuid4())  # Unique identifier for the analyzer\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "#    x_ms_useragent=\"azure-ai-tag-with-content-understanding-python/video_tag\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(\n",
    "    ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85cf38",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video content\n",
    "It might take some time depending on the video length. Try with short videos to get results faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)\n",
    "\n",
    "# Wait for the analysis to complete and get the content analysis result\n",
    "video_cu_result = cu_client.poll_result(\n",
    "    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result)\n",
    "\n",
    "# Optional - Delete the analyzer if it is no longer needed\n",
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95188194",
   "metadata": {},
   "source": [
    "## Aggregate tags from each segment to generate video tags\n",
    "\n",
    "ChatGPT will be used to remove duplicate tags which are semantically similar across segments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.utility import OpenAIAssistant, aggregate_tags\n",
    "\n",
    "# Create an OpenAI Assistant to interact with Azure OpenAI\n",
    "openai_assistant = OpenAIAssistant(\n",
    "    aoai_end_point=AZURE_OPENAI_ENDPOINT,\n",
    "    aoai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    aoai_api_key=None,\n",
    ")\n",
    "\n",
    "# Aggregate tags using the video segment result from Azure Content Understanding\n",
    "tag_result = aggregate_tags(video_cu_result, openai_assistant)\n",
    "\n",
    "tag_result.tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
